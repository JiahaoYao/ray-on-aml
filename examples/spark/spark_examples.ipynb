{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Examples of using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that Spark only works with ray-on-aml version 0.1.8 or lower for now.\n",
    "#do pip install ray-on-aml==0.1.8 to run the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment,ScriptRunConfig\n",
    "# from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.environment import Environment\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray_on_aml.core import Ray_On_AML\n",
    "ws = Workspace.from_config()\n",
    "ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"d12-v2-ssh\", maxnode=5)\n",
    "ray = ray_on_aml.getRay()\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shut down when you're done\n",
    "ray_on_aml.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Delta Lake data from ADLS Gen2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account_name =\"adlsdatalakegen6\"\n",
    "storage_account_key=ws.get_default_keyvault().get_secret(\"adlsdatalakegen6\")\n",
    "additional_spark_configs ={f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\":f\"{storage_account_key}\"}\n",
    "spark = ray_on_aml.getSpark(executor_cores =3,num_executors =3 ,executor_memory='10GB', additional_spark_configs=additional_spark_configs)\n",
    "#Number of nodes (including head node) can be set as number of executor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"abfss://mltraining@adlsdatalakegen6.dfs.core.windows.net/ISDWeatherDelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()\n",
    "73,696,631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Koalas (pyspark Pandas) API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.pandas import read_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delta = read_delta(\"abfss://mltraining@adlsdatalakegen6.dfs.core.windows.net/ISDWeatherDelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_delta[\"snowDepth\"].plot.area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1643687057242
    }
   },
   "outputs": [],
   "source": [
    "test_delta.groupby(\"stationName\").count().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1643687072263
    }
   },
   "outputs": [],
   "source": [
    "adls_data = spark.read.format(\"delta\").load(\"abfss://mltraining@adlsdatalakegen6.dfs.core.windows.net/ISDWeatherDelta\")\n",
    "adls_data.groupby(\"stationName\").count().head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synapse SQL Pool Data Access (Dedicated or Serverless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = \"jdbc:sqlserver://sy2qwhqqkv7eacsws1-ondemand.sql.azuresynapse.net:1433\"\n",
    "database_name = \"mydbname\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"\"\"\n",
    "SELECT TOP 10 *\n",
    "FROM OPENROWSET\n",
    "  (\n",
    "      BULK 'csv/population/*.csv',\n",
    "      DATA_SOURCE = 'SqlOnDemandDemo',\n",
    "      FORMAT = 'CSV', PARSER_VERSION = '2.0'\n",
    "  )\n",
    "WITH\n",
    "  (\n",
    "      country_code VARCHAR (5)\n",
    "    , country_name VARCHAR (100)\n",
    "    , year smallint\n",
    "    , population bigint\n",
    "  ) AS r\n",
    "WHERE\n",
    "  country_name = 'Luxembourg' AND year = 2017\n",
    "\n",
    "\"\"\"\n",
    "username = \"azureuser\"\n",
    "password = \"abcd@12345\" # Please specify password here\n",
    "\n",
    "\n",
    "jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"query\", table_name) \\\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "        .option(\"user\", username) \\\n",
    "        .option(\"password\", password).load()\n",
    "\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML with Collaborative Filtering \n",
    "(https://spark.apache.org/docs/3.2.1/ml-collaborative-filtering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "lines = spark.read.text(\"abfss://mltraining@adlsdatalakegen6.dfs.core.windows.net/mllib/sample_movielens_ratings.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML with Spark & XGBoost \n",
    "(transformation script and example are copied from https://github.com/oap-project/raydp/tree/master/python/raydp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading NYC taxi\n",
    "# nyc_taxi_df = spark.read.format(\"parquet\").load(\"wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "# XGBoost on ray is needed to run this example.\n",
    "# Please refer to https://docs.ray.io/en/latest/xgboost-ray.html to install it.\n",
    "from xgboost_ray import RayDMatrix, train, RayParams\n",
    "import raydp\n",
    "from raydp.utils import random_split\n",
    "from raydp.spark import RayMLDataset\n",
    "\n",
    "\n",
    "from spark.data_process import nyc_taxi_preprocess, NYC_TRAIN_CSV\n",
    "\n",
    "# connect to ray cluster\n",
    "# ray.init(address='auto')\n",
    "# # After ray.init, you can use the raydp api to get a spark session\n",
    "\n",
    "data = spark.read.format(\"csv\").option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(NYC_TRAIN_CSV)\n",
    "# Set spark timezone for processing datetime\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "# Transform the dataset\n",
    "data = nyc_taxi_preprocess(data)\n",
    "# Split data into train_dataset and test_dataset\n",
    "train_df, test_df = random_split(data, [0.9, 0.1], 0)\n",
    "# Convert spark dataframe into ray dataset\n",
    "train_dataset = ray.data.from_spark(train_df)\n",
    "test_dataset = ray.data.from_spark(test_df)\n",
    "# Then convert them into DMatrix used by xgboost\n",
    "dtrain = RayDMatrix(train_dataset, label='fare_amount')\n",
    "dtest = RayDMatrix(test_dataset, label='fare_amount')\n",
    "# Configure the XGBoost model\n",
    "config = {\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "evals_result = {}\n",
    "# Train the model\n",
    "bst = train(\n",
    "        config,\n",
    "        dtrain,\n",
    "        evals=[(dtest, \"eval\")],\n",
    "        evals_result=evals_result,\n",
    "        ray_params=RayParams(max_actor_restarts=2, num_actors=2, cpus_per_actor=2),\n",
    "        num_boost_round=10)\n",
    "# print evaluation stats\n",
    "print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('dlresearch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8858a4df92b06e9052bc306608e3218c33233584bc6448961c72d65ba55843de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
