{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for Interactive use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade ray-on-aml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1639183943936
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from azureml.core import Workspace, Experiment, Environment, Datastore, Dataset, ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "# from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import platform\n",
    "import sys\n",
    "# sys.path.append(\"../\") # go to parent dir\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1639185778952
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank returned is  None\n",
      "rank returned is  None\n",
      "azureml_py38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 22:39:08,731\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 22:39:25,287\tINFO worker.py:842 -- Connecting to existing Ray cluster at address: 10.0.0.13:6379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n",
      "Waiting: Cluster status is in  Queued\n"
     ]
    }
   ],
   "source": [
    "#You can pre-provision \"worker-cpu-v3\" in the same vnet with your compute instance\n",
    "from ray_on_aml.core import Ray_On_AML\n",
    "ws = Workspace.from_config()\n",
    "ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"worker-cpu-v3\", additional_pip_packages=['torch==1.10.0'])\n",
    "_, ray = ray_on_aml.getRay()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1639186321954
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node:10.0.0.17': 1.0,\n",
       " 'CPU': 26.0,\n",
       " 'object_store_memory': 31006511716.0,\n",
       " 'memory': 87246477520.0,\n",
       " 'node:10.0.0.20': 1.0,\n",
       " 'node:10.0.0.18': 1.0,\n",
       " 'node:10.0.0.21': 1.0,\n",
       " 'node:10.0.0.22': 1.0,\n",
       " 'GPU': 1.0,\n",
       " 'accelerator_type:K80': 1.0,\n",
       " 'node:10.0.0.11': 1.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask on Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gather": {
     "logged": 1639183635290
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 05:32:12,050\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2021-12-13 05:32:12,054\tWARNING services.py:1816 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 5310255104 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4318.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>885.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6895.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>8404.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>8000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>7781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>7748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>3398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>5400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>976 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       grade\n",
       "age         \n",
       "6     4318.0\n",
       "45     885.0\n",
       "46    6405.0\n",
       "77    6895.0\n",
       "81    8404.0\n",
       "...      ...\n",
       "9895  8000.0\n",
       "9915  7781.0\n",
       "9951  7748.0\n",
       "9969  3398.0\n",
       "9992  5400.0\n",
       "\n",
       "[976 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.init()\n",
    "from ray.util.dask import ray_dask_get\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "dask.config.set(scheduler=ray_dask_get)\n",
    "d_arr = da.from_array(np.random.randint(0, 1000, size=(256, 256)))\n",
    "\n",
    "# The Dask scheduler submits the underlying task graph to Ray.\n",
    "d_arr.mean().compute(scheduler=ray_dask_get)\n",
    "\n",
    "# Set the scheduler to ray_dask_get in your config so you don't have to\n",
    "# specify it on each compute call.\n",
    "\n",
    "df = dd.from_pandas(\n",
    "    pd.DataFrame(\n",
    "        np.random.randint(0, 10000, size=(1024, 2)), columns=[\"age\", \"grade\"]),\n",
    "    npartitions=2)\n",
    "df.groupby([\"age\"]).mean().compute()\n",
    "\n",
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gather": {
     "logged": 1639186778862
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:09<00:00,  1.64it/s]\n",
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:05<00:00,  3.12it/s]\n",
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:04<00:00,  3.28it/s]\n",
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:04<00:00,  3.35it/s]\n",
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:05<00:00,  3.07it/s]\n",
      "Metadata Fetch Progress: 100%|██████████| 16/16 [00:04<00:00,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#dask\n",
    "\n",
    "# import ray\n",
    "from ray.util.dask import ray_dask_get\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from azureml.core import Workspace, Dataset, Model\n",
    "from adlfs import AzureBlobFileSystem\n",
    "# account_key = ws.get_default_keyvault().get_secret(\"adls7-account-key\")\n",
    "# account_name=\"adlsgen7\"\n",
    "# abfs = AzureBlobFileSystem(account_name=\"adlsgen7\",account_key=account_key,  container_name=\"mltraining\")\n",
    "abfs2 = AzureBlobFileSystem(account_name=\"azureopendatastorage\",  container_name=\"isdweatherdatacontainer\")\n",
    "\n",
    "\n",
    "storage_options={'account_name': account_name, 'account_key': account_key}\n",
    "\n",
    "# ddf = dd.read_parquet('az://mltraining/ISDWeatherDelta/year2008', storage_options=storage_options)\n",
    "\n",
    "data = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2012/\"], filesystem=abfs2)\n",
    "data1 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2015/\"], filesystem=abfs2)\n",
    "data2 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2010/\"], filesystem=abfs2)\n",
    "data3 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2009/\"], filesystem=abfs2)\n",
    "data4 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2011/\"], filesystem=abfs2)\n",
    "data5 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2013/\"], filesystem=abfs2)\n",
    "data6 = ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2014/\"], filesystem=abfs2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690211698"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data =data.union(data1).union(data2).union(data3).union(data4).union(data5).union(data6)\n",
    "all_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('quantiles-1-d7cd6e40aa11b038fe5ca5a773d25ac8', 298) pid=906, ip=10.0.0.22)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('from-delayed-37bcded3107f2c11146104d14a59d0ad', 368) pid=80603)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('quantiles-1-6e697189017d81f71030f5e8bf0ee8a2', 189) pid=263, ip=10.0.0.22)\u001b[0m \n",
      "\u001b[2m\u001b[36m(dask:('quantiles-1-040e133c45167fb786e4c5b3a28faf1c', 66) pid=905, ip=10.0.0.22)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:block_to_df-c2e390a0-5836-4fea-82e3-19ef23ae7d25 pid=198, ip=10.0.0.22)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('dropna-ba9cf0277682a5953a58639127dd84bd', 192) pid=2029, ip=10.0.0.20)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('dataframe-count-chunk-794783190c86b2d3594773c9ffe513ce', 0, 186, 0) pid=195, ip=10.0.0.20)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('quantiles-1-046f250e7af81064a522328b34088f5b', 410) pid=29527, ip=10.0.0.20)\u001b[0m \n",
      "\u001b[2m\u001b[36m(dask:('dropna-d24804d88d961fea6af2f5601cea6261', 493) pid=2574, ip=10.0.0.17)\u001b[0m \n",
      "\u001b[2m\u001b[36m(dask:('dropna-8598322c3d00cd5ed4009f78d212d981', 303) pid=2574, ip=10.0.0.17)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('from-delayed-37bcded3107f2c11146104d14a59d0ad', 429) pid=374, ip=10.0.0.17)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('quantiles-1-24c4de00c62d75056557559435285835', 561) pid=2574, ip=10.0.0.17)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(dask:('dropna-d4e810db3c5e478a2250add67fbce6c8', 506) pid=200, ip=10.0.0.17)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           latitude     longitude     elevation     windAngle     windSpeed  \\\n",
      "count  6.902117e+08  6.902117e+08  6.902117e+08  5.414428e+08  5.624939e+08   \n",
      "mean   3.796754e+01 -4.137369e+01  4.173534e+02  1.642972e+02  3.452701e+00   \n",
      "std    2.068093e+01  7.837449e+01  6.445339e+02  1.146524e+02  2.888769e+00   \n",
      "min   -9.000000e+01 -1.799830e+02 -3.500000e+02  0.000000e+00  0.000000e+00   \n",
      "25%    3.395600e+01 -9.571000e+01  5.000000e+01  8.000000e+01  2.000000e+00   \n",
      "50%    4.155400e+01 -7.619200e+01  2.100000e+02  1.900000e+02  3.100000e+00   \n",
      "75%    4.918300e+01  1.668300e+01  5.500000e+02  2.800000e+02  5.700000e+00   \n",
      "max    8.733300e+01  9.999990e+02  9.999000e+03  3.600000e+02  9.000000e+01   \n",
      "\n",
      "        temperature  seaLvlPressure  presentWeatherIndicator  \\\n",
      "count  6.779744e+08    2.284912e+08             6.001119e+07   \n",
      "mean   1.185499e+01    1.014672e+03             2.821610e+01   \n",
      "std    1.247398e+01    9.456261e+00             2.883805e+01   \n",
      "min   -8.860000e+01    8.727000e+02             0.000000e+00   \n",
      "25%    8.700000e+00    1.011600e+03             1.000000e+01   \n",
      "50%    1.610000e+01    1.017200e+03             1.300000e+01   \n",
      "75%    2.780000e+01    1.026000e+03             6.100000e+01   \n",
      "max    6.120000e+01    1.090000e+03             9.900000e+01   \n",
      "\n",
      "       pastWeatherIndicator    precipTime   precipDepth     snowDepth  \\\n",
      "count          2.584350e+07  1.329948e+08  1.329948e+08  3.270731e+06   \n",
      "mean           4.273148e+00  5.674848e+00  2.966430e+02  2.157168e+01   \n",
      "std            2.998129e+00  8.696258e+00  1.669186e+03  3.609434e+01   \n",
      "min            0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%            2.000000e+00  1.000000e+00  0.000000e+00  3.000000e+00   \n",
      "50%            6.000000e+00  1.000000e+00  0.000000e+00  2.000000e+01   \n",
      "75%            8.000000e+00  6.000000e+00  7.000000e+00  6.500000e+01   \n",
      "max            9.000000e+00  9.900000e+01  9.999000e+03  9.980000e+02   \n",
      "\n",
      "               year           day      version         month  \n",
      "count  6.902117e+08  6.902117e+08  690211698.0  6.902117e+08  \n",
      "mean   2.012449e+03  1.572335e+01          1.0  6.565310e+00  \n",
      "std    1.912755e+00  8.809711e+00          0.0  3.461218e+00  \n",
      "min    2.009000e+03  1.000000e+00          1.0  1.000000e+00  \n",
      "25%    2.011000e+03  8.000000e+00          1.0  4.000000e+00  \n",
      "50%    2.013000e+03  1.600000e+01          1.0  7.000000e+00  \n",
      "75%    2.014000e+03  2.400000e+01          1.0  1.000000e+01  \n",
      "max    2.015000e+03  3.100000e+01          1.0  1.200000e+01  \n",
      "duration  1361.3095450401306\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "all_data_dask = all_data.to_dask().describe().compute()\n",
    "print(all_data_dask)\n",
    "stop = time.time()\n",
    "print(\"duration \", (stop-start))\n",
    "#717s for single machine nc6\n",
    "#duration  1854.3798034191132 cluster 1st time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Ray Tune for distributed ML tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "gather": {
     "logged": 1639186335816
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "# import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # In this example, we don't change the model architecture\n",
    "        # due to simplicity.\n",
    "        self.conv1 = nn.Conv2d(1, 3, kernel_size=3)\n",
    "        self.fc = nn.Linear(192, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 3))\n",
    "        x = x.view(-1, 192)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "# Change these values if you want the training to run quicker or slower.\n",
    "EPOCH_SIZE = 512\n",
    "TEST_SIZE = 256\n",
    "\n",
    "def train(model, optimizer, train_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We set this just for the example to run quickly.\n",
    "        if batch_idx * len(data) > EPOCH_SIZE:\n",
    "            return\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            # We set this just for the example to run quickly.\n",
    "            if batch_idx * len(data) > TEST_SIZE:\n",
    "                break\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "def train_mnist(config):\n",
    "    # Data Setup\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.1307, ), (0.3081, ))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=True, download=True, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms),\n",
    "        batch_size=64,\n",
    "        shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"])\n",
    "    for i in range(10):\n",
    "        train(model, optimizer, train_loader)\n",
    "        acc = test(model, test_loader)\n",
    "\n",
    "        # Send the current training result back to Tune\n",
    "        tune.report(mean_accuracy=acc)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            # This saves the model to the trial directory\n",
    "            torch.save(model.state_dict(), \"./model.pth\")\n",
    "search_space = {\n",
    "    \"lr\": tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n",
    "    \"momentum\": tune.uniform(0.01, 0.09)\n",
    "}\n",
    "\n",
    "# Uncomment this to enable distributed execution\n",
    "# ray.shutdown()\n",
    "# ray.init(address=\"auto\",ignore_reinit_error=True)\n",
    "# ray.init(address =f'ray://{headnode_private_ip}:10001',allow_multiple=True,ignore_reinit_error=True )\n",
    "# Download the dataset first\n",
    "datasets.MNIST(\"~/data\", train=True, download=True)\n",
    "\n",
    "analysis = tune.run(train_mnist, config=search_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1639186714110
    }
   },
   "outputs": [],
   "source": [
    " import sklearn.datasets\n",
    " import sklearn.metrics\n",
    " from sklearn.model_selection import train_test_split\n",
    " import xgboost as xgb\n",
    "\n",
    " from ray import tune\n",
    "\n",
    "\n",
    " def train_breast_cancer(config):\n",
    "     # Load dataset\n",
    "     data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "     # Split into train and test set\n",
    "     train_x, test_x, train_y, test_y = train_test_split(\n",
    "         data, labels, test_size=0.25)\n",
    "     # Build input matrices for XGBoost\n",
    "     train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "     test_set = xgb.DMatrix(test_x, label=test_y)\n",
    "     # Train the classifier\n",
    "     results = {}\n",
    "     xgb.train(\n",
    "         config,\n",
    "         train_set,\n",
    "         evals=[(test_set, \"eval\")],\n",
    "         evals_result=results,\n",
    "         verbose_eval=False)\n",
    "     # Return prediction accuracy\n",
    "     accuracy = 1. - results[\"eval\"][\"error\"][-1]\n",
    "     tune.report(mean_accuracy=accuracy, done=True)\n",
    "\n",
    "\n",
    " config = {\n",
    "     \"objective\": \"binary:logistic\",\n",
    "     \"eval_metric\": [\"logloss\", \"error\"],\n",
    "     \"max_depth\": tune.randint(1, 9),\n",
    "     \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "     \"subsample\": tune.uniform(0.5, 1.0),\n",
    "     \"eta\": tune.loguniform(1e-4, 1e-1)\n",
    " }\n",
    " analysis = tune.run(\n",
    "     train_breast_cancer,\n",
    "     resources_per_trial={\"cpu\": 1},\n",
    "     config=config,\n",
    "     num_samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown when not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_on_aml.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ray on Job Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1639179342786
    }
   },
   "outputs": [
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "\n",
        "# base_conda_dep =['adlfs>=2021.10.0','pytorch','matplotlib','torchvision','pip']\n",
        "# base_pip_dep = ['sklearn','xgboost','lightgbm','ray[default]==1.9.0', 'xgboost_ray', 'dask','pyarrow>=6.0.1', 'azureml-mlflow']\n",
        "compute_cluster = 'worker-cpu-v3'\n",
        "maxnode =5\n",
        "vm_size='STANDARD_DS3_V2'\n",
        "vnet='rayvnet'\n",
        "subnet='default'\n",
        "exp ='ray_on_aml_job'\n",
        "ws_detail = ws.get_details()\n",
        "ws_rg = ws_detail['id'].split(\"/\")[4]\n",
        "vnet_rg=None\n",
        "try:\n",
        "    ray_cluster = ComputeTarget(workspace=ws, name=compute_cluster)\n",
        "\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    if vnet_rg is None:\n",
        "        vnet_rg = ws_rg\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
        "                                                        min_nodes=0, max_nodes=maxnode,\n",
        "                                                        vnet_resourcegroup_name=vnet_rg,\n",
        "                                                        vnet_name=vnet,\n",
        "                                                        subnet_name=subnet)\n",
        "    ray_cluster = ComputeTarget.create(ws, compute_cluster, compute_config)\n",
        "\n",
        "    ray_cluster.wait_for_completion(show_output=True)\n",
        "\n",
        "\n",
        "# python_version = [\"python=\"+platform.python_version()]\n",
        "\n",
        "\n",
        "\n",
        "# conda_packages = python_version+base_conda_dep\n",
        "# pip_packages = base_pip_dep \n",
        "\n",
        "# conda_dep = CondaDependencies()\n",
        "\n",
        "# rayEnv = Environment(name=\"rayEnv\")\n",
        "rayEnv = Environment.get(ws, \"rayEnv\", version=18)\n",
        "# for conda_package in conda_packages:\n",
        "#     conda_dep.add_conda_package(conda_package)\n",
        "\n",
        "# for pip_package in pip_packages:\n",
        "#     conda_dep.add_pip_package(pip_package)\n",
        "\n",
        "# # Adds dependencies to PythonSection of myenv\n",
        "# rayEnv.python.conda_dependencies=conda_dep\n",
        "\n",
        "src = ScriptRunConfig(source_directory='job',\n",
        "                script='aml_job.py',\n",
        "                environment=rayEnv,\n",
        "                compute_target=ray_cluster,\n",
        "                distributed_job_config=PyTorchConfiguration(node_count=maxnode),\n",
        "                    # arguments = [\"--master_ip\",master_ip]\n",
        "                )\n",
        "run = Experiment(ws, exp).submit(src)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "compute_cluster = 'worker-cpu-v3'\n",
    "maxnode =5\n",
    "vm_size='STANDARD_DS3_V2'\n",
    "vnet='rayvnet'\n",
    "subnet='default'\n",
    "exp ='ray_on_aml_job'\n",
    "ws_detail = ws.get_details()\n",
    "ws_rg = ws_detail['id'].split(\"/\")[4]\n",
    "vnet_rg=None\n",
    "try:\n",
    "    ray_cluster = ComputeTarget(workspace=ws, name=compute_cluster)\n",
    "\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    if vnet_rg is None:\n",
    "        vnet_rg = ws_rg\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                        min_nodes=0, max_nodes=maxnode,\n",
    "                                                        vnet_resourcegroup_name=vnet_rg,\n",
    "                                                        vnet_name=vnet,\n",
    "                                                        subnet_name=subnet)\n",
    "    ray_cluster = ComputeTarget.create(ws, compute_cluster, compute_config)\n",
    "\n",
    "    ray_cluster.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "rayEnv = Environment.from_conda_specification(name = \"rayEnv\",\n",
    "                                             file_path = \"../examples/conda_env.yml\")\n",
    "\n",
    "# rayEnv = Environment.get(ws, \"rayEnv\", version=19)\n",
    "\n",
    "\n",
    "src = ScriptRunConfig(source_directory='../examples/job',\n",
    "                script='aml_job.py',\n",
    "                environment=rayEnv,\n",
    "                compute_target=ray_cluster,\n",
    "                distributed_job_config=PyTorchConfiguration(node_count=maxnode),\n",
    "                    # arguments = [\"--master_ip\",master_ip]\n",
    "                )\n",
    "run = Experiment(ws, exp).submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}