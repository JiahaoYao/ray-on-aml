{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Quick examples to demostrate AML Ray/Dask cluster usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive use cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install at this your conda notebook environment to run interactive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade ray==2.2.0 ray[air]==2.2.0 ray[data]==2.2.0 azure-ai-ml ray-on-aml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start ML client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1639183943936
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml import command, Input\n",
    "from azure.identity import DefaultAzureCredential\n",
    "# Enter details of your AML workspace\n",
    "subscription_id = \"840b5c5c-3f4a-459a-94fc-6bad2a969f9d\"\n",
    "resource_group = \"ml\"\n",
    "workspace = \"ws02ent\"\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please make sure to create compute cluster in the same vnet with your compute instance. You need to have vnet, otherwise compute cannot communicate with each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1639185778952
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'ray-on-aml-3490264930' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'ray-on-aml-3490264930' will not be used for anonymous registration\n",
      "\u001b[32mUploading .tmp (0.01 MBs): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6679/6679 [00:00<00:00, 97620.47it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting cluster to start and return head node's ip\n",
      "............................................................................................................................................\n",
      " cluster is ready, head node ip  10.0.0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet, ip=10.0.0.8)\u001b[0m /azureml-envs/azureml_c0eb32c1b90d5f184f6500831f6bfab1/lib/python3.10/site-packages/ray/dashboard/agent.py:51: DeprecationWarning: There is no current event loop\n",
      "\u001b[2m\u001b[33m(raylet, ip=10.0.0.8)\u001b[0m   aiogrpc.init_grpc_aio()\n"
     ]
    }
   ],
   "source": [
    "from ray_on_aml.core import Ray_On_AML\n",
    "\n",
    "# from src.ray_on_aml.core import Ray_On_AML\n",
    "\n",
    "import logging\n",
    "ray_on_aml =Ray_On_AML(ml_client=ml_client, compute_cluster =\"d13\")\n",
    "#Note that if you need to customize the pip installation of the cluster, you also needs to support the ray package e.g. ray[data] which \n",
    "#match the version of the ray package(s) in your compute instance. If you don't specify pip_packages then ray[default] is inserted \n",
    "#automatically\n",
    "\n",
    "# For use as client mode, uncomment these lines\n",
    "ray = ray_on_aml.getRay(num_node=2,pip_packages=[\"ray[air]==2.2.0\",\"ray[data]==2.2.0\",\"torch==1.13.0\",\"fastparquet==2022.12.0\", \n",
    "\"azureml-mlflow==1.48.0\", \"pyarrow==6.0.1\", \"dask==2022.12.0\", \"adlfs==2022.11.2\", \"fsspec==2022.11.0\"])\n",
    "client = ray.init(f\"ray://{ray_on_aml.headnode_private_ip}:10001\")\n",
    "\n",
    "#use CI as head node\n",
    "\n",
    "# ray = ray_on_aml.getRay(ci_is_head=True, num_node=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CPU': 16.0,\n",
       " 'node:10.0.0.6': 1.0,\n",
       " 'node:10.0.0.8': 1.0,\n",
       " 'memory': 87326674944.0,\n",
       " 'object_store_memory': 24481313586.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask on Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can use Dask on this Ray cluster by telling Dask to use Ray as the scheduler. By doing this, you will have a cluster with both Dask and Ray without having to setup them saperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1639183635290
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendorID                19233765\n",
       "lpepPickupDatetime      19233765\n",
       "lpepDropoffDatetime     19233765\n",
       "passengerCount          19233765\n",
       "tripDistance            19233765\n",
       "puLocationId                   0\n",
       "doLocationId                   0\n",
       "pickupLongitude         19233765\n",
       "pickupLatitude          19233765\n",
       "dropoffLongitude        19233765\n",
       "dropoffLatitude         19233765\n",
       "rateCodeID              19233765\n",
       "storeAndFwdFlag         19233765\n",
       "paymentType             19233765\n",
       "fareAmount              19233765\n",
       "extra                   19233765\n",
       "mtaTax                  19233765\n",
       "improvementSurcharge    19233765\n",
       "tipAmount               19233765\n",
       "tollsAmount             19233765\n",
       "ehailFee                       0\n",
       "totalAmount             19233765\n",
       "tripType                19233693\n",
       "puYear                  19233765\n",
       "puMonth                 19233765\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling up date with Dask dataframe API.\n",
    "#Please make sure you have pandas version 1.4+ and restart to run this successfully.\n",
    "# pip install pandas==1.4.2\n",
    "import dask\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "enable_dask_on_ray()\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "storage_options = {'account_name': 'azureopendatastorage'}\n",
    "ddf = dd.read_parquet('az://nyctlc/green/puYear=2015/puMonth=*/*.parquet', storage_options=storage_options)\n",
    "ddf.count().compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1639186778862
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_get_read_tasks pid=589)\u001b[0m [dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116341246"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ray also support Ray dataset. You can read into ray dataset then convert to Dask or other ML format which is convenient for ML training.https://docs.ray.io/en/latest/data/dataset.html\n",
    "# you may need to upgrade pyarrow to run this successfully\n",
    "from adlfs import AzureBlobFileSystem\n",
    "\n",
    "abfs = AzureBlobFileSystem(account_name=\"azureopendatastorage\",  container_name=\"isdweatherdatacontainer\")\n",
    "#if read all years and months\n",
    "# data = ray.data.read_parquet(\"az://isdweatherdatacontainer/ISDWeather//\", filesystem=abfs)\n",
    "data =ray.data.read_parquet([\"az://isdweatherdatacontainer/ISDWeather/year=2015/\"], filesystem=abfs)\n",
    "data.count()\n",
    "# 1,584,481,119 is the count for all data \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount remote data to your ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.disconnect()\n",
    "ray_on_aml.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of using input and output for interactive job. You need to define adlsstore0001 in your Azure ML workspace first\n",
    "from azure.ai.ml import command, Input, Output\n",
    "from ray_on_aml.core import Ray_On_AML\n",
    "import logging\n",
    "ray_on_aml =Ray_On_AML(ml_client=ml_client, compute_cluster =\"ds11\", verbosity=logging.INFO )\n",
    "\n",
    "inputs={\n",
    "\n",
    "    \"ISDWeather\": Input(\n",
    "        type=\"uri_folder\",\n",
    "        path=\"azureml://datastores/adlsstore0001/paths/ISDWeather/year=2008\",\n",
    "    )\n",
    "}\n",
    "\n",
    "outputs={\n",
    "    \"output1\": Output(\n",
    "        type=\"uri_folder\",\n",
    "        path=\"azureml://datastores/adlsstore0001/paths/dev\",\n",
    "    ),\n",
    "    \"output2\": Output(\n",
    "        type=\"uri_folder\",\n",
    "        path=\"azureml://datastores/adlsstore0001/paths/dev\",\n",
    "    )\n",
    "}\n",
    "\n",
    "ray = ray_on_aml.getRay(inputs = inputs,outputs=outputs, num_node=2,\n",
    "pip_packages=[\"ray[air]==2.2.0\",\"ray[data]==2.2.0\",\"torch==1.13.0\",\"fastparquet==2022.12.0\", \n",
    "\"azureml-mlflow==1.48.0\", \"pyarrow==6.0.1\", \"dask==2022.2.0\", \"adlfs==2022.11.2\", \"fsspec==2022.11.0\"])\n",
    "client = ray.init(f\"ray://{ray_on_aml.headnode_private_ip}:10001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ray.data.read_parquet(ray_on_aml.mount_points['ISDWeather'])\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ray Tune for distributed ML tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639186714110
    }
   },
   "outputs": [],
   "source": [
    " import sklearn.datasets\n",
    " import sklearn.metrics\n",
    " from sklearn.model_selection import train_test_split\n",
    " import xgboost as xgb\n",
    "\n",
    " from ray import tune\n",
    "\n",
    "\n",
    " def train_breast_cancer(config):\n",
    "     # Load dataset\n",
    "     data, labels = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "     # Split into train and test set\n",
    "     train_x, test_x, train_y, test_y = train_test_split(\n",
    "         data, labels, test_size=0.25)\n",
    "     # Build input matrices for XGBoost\n",
    "     train_set = xgb.DMatrix(train_x, label=train_y)\n",
    "     test_set = xgb.DMatrix(test_x, label=test_y)\n",
    "     # Train the classifier\n",
    "     results = {}\n",
    "     xgb.train(\n",
    "         config,\n",
    "         train_set,\n",
    "         evals=[(test_set, \"eval\")],\n",
    "         evals_result=results,\n",
    "         verbose_eval=False)\n",
    "     # Return prediction accuracy\n",
    "     accuracy = 1. - results[\"eval\"][\"error\"][-1]\n",
    "     tune.report(mean_accuracy=accuracy, done=True)\n",
    "\n",
    "\n",
    " config = {\n",
    "     \"objective\": \"binary:logistic\",\n",
    "     \"eval_metric\": [\"logloss\", \"error\"],\n",
    "     \"max_depth\": tune.randint(1, 9),\n",
    "     \"min_child_weight\": tune.choice([1, 2, 3]),\n",
    "     \"subsample\": tune.uniform(0.5, 1.0),\n",
    "     \"eta\": tune.loguniform(1e-4, 1e-1)\n",
    " }\n",
    " analysis = tune.run(\n",
    "     train_breast_cancer,\n",
    "     resources_per_trial={\"cpu\": 1},\n",
    "     config=config,\n",
    "     num_samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed XGBoost https://docs.ray.io/en/latest/xgboost-ray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost_ray import RayXGBClassifier, RayParams\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "clf = RayXGBClassifier(\n",
    "    n_jobs=10,  # In XGBoost-Ray, n_jobs sets the number of actors\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# scikit-learn API will automatically conver the data\n",
    "# to RayDMatrix format as needed.\n",
    "# You can also pass X as a RayDMatrix, in which case\n",
    "# y will be ignored.\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred_ray = clf.predict(X_test)\n",
    "print(pred_ray.shape)\n",
    "\n",
    "pred_proba_ray = clf.predict_proba(X_test)\n",
    "print(pred_proba_ray.shape)\n",
    "\n",
    "# It is also possible to pass a RayParams object\n",
    "# to fit/predict/predict_proba methods - will override\n",
    "# n_jobs set during initialization\n",
    "\n",
    "clf.fit(X_train, y_train, ray_params=RayParams(num_actors=10))\n",
    "\n",
    "pred_ray = clf.predict(X_test, ray_params=RayParams(num_actors=10))\n",
    "print(pred_ray.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "train_x, train_y = load_breast_cancer(return_X_y=True)\n",
    "train_set = RayDMatrix(train_x, train_y)\n",
    "\n",
    "evals_result = {}\n",
    "bst = train(\n",
    "    {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    train_set,\n",
    "    evals_result=evals_result,\n",
    "    evals=[(train_set, \"train\")],\n",
    "    verbose_eval=False,\n",
    "    ray_params=RayParams(\n",
    "        num_actors=10,  # Number of remote actors\n",
    "        cpus_per_actor=1))\n",
    "\n",
    "bst.save_model(\"model.xgb\")\n",
    "print(\"Final training error: {:.4f}\".format(\n",
    "    evals_result[\"train\"][\"error\"][-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install library at compute instance: pip install gym,dm-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install additional library at Ray cluster\n",
    "ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"d15-v2\",additional_pip_packages=['torch==1.10.0', 'torchvision', 'sklearn', 'pyspark','gym==0.2.1','dm-tree','scikit-image','opencv-python','tensorflow'], maxnode=1)\n",
    "ray = ray_on_aml.getRay()\n",
    "time.sleep(20)\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RL algorithm (Trainer) we would like to use.\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "# Configure the algorithm.\n",
    "config = {\n",
    "    # Environment (RLlib understands openAI gym registered strings).\n",
    "    \"env\": \"Taxi-v3\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 2,\n",
    "    # Change this to \"framework: torch\", if you are using PyTorch.\n",
    "    # Also, use \"framework: tf2\" for tf2.x eager execution.\n",
    "    \"framework\": \"torch\",\n",
    "    # Tweak the default model provided automatically by RLlib,\n",
    "    # given the environment's observation- and action spaces.\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    # Set up a separate evaluation worker set for the\n",
    "    # `trainer.evaluate()` call after training (see below).\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = PPOTrainer(config=config)\n",
    "\n",
    "# Run it for n training iterations. A training iteration includes\n",
    "# parallel sample collection by the environment workers as well as\n",
    "# loss calculation on the collected batch and a model update.\n",
    "for _ in range(3):\n",
    "    print(trainer.train())\n",
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's\n",
    "# output).\n",
    "trainer.evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown interactive cluster when not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_on_aml.shutdown()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray on Job Cluster with GPU (you don't need interactive Ray cluster to be on to submit the AML job )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1639179342786
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment,ScriptRunConfig\n",
    "# from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DockerConfiguration,RunConfiguration\n",
    "\n",
    "#Remember the AML job has to have distribted setings (MPI type) for ray-on-aml to work correctly.\n",
    "ws = Workspace.from_config()\n",
    "compute_cluster = 'gpunc6' #This can be another cluster different from the interactive cluster. \n",
    "ray_cluster = ComputeTarget(workspace=ws, name=compute_cluster)\n",
    "\n",
    "aml_run_config_ml = RunConfiguration(communicator='OpenMpi')\n",
    "docker_config = DockerConfiguration(use_docker=True, shm_size='48gb')\n",
    "\n",
    "\n",
    "rayEnv = Environment.from_conda_specification(name = \"RLEnv\",\n",
    "                                             file_path = \"conda_env.yml\")\n",
    "rayEnv.docker.base_image = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220329.v1\"\n",
    "\n",
    "aml_run_config_ml.target = ray_cluster\n",
    "aml_run_config_ml.node_count = 2\n",
    "aml_run_config_ml.environment = rayEnv\n",
    "aml_run_config_ml.docker =docker_config\n",
    "\n",
    "src = ScriptRunConfig(source_directory='../examples/job',\n",
    "                    script='aml_job.py',\n",
    "                    run_config = aml_run_config_ml,\n",
    "                   )\n",
    "\n",
    "run = Experiment(ws, \"rl_on_aml_job\").submit(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
